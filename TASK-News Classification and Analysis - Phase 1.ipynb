{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser=webdriver.Chrome('chromedriver.exe')\n",
    "browser.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get('https://timesofindia.indiatimes.com/archive/year-2018,month-9.cms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## collecting all dates ##################33\n",
    "div=browser.find_element_by_id(\"calenderdiv\")\n",
    "dates=[]\n",
    "for i in div.find_elements_by_xpath(\".//table/tbody/tr\"):\n",
    "    for k in i.find_elements_by_xpath(\".//td/a\"):\n",
    "        date_exist=False\n",
    "        date=k.get_attribute('href')\n",
    "        ### searching if date already present in dates list\n",
    "        for x in dates:\n",
    "            if(x==date):date_exist=True\n",
    "        ## adding date if date not present in list\n",
    "        if date_exist == False:dates.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://timesofindia.indiatimes.com/2018/9/1/archivelist/year-2018,month-9,starttime-43344.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/2/archivelist/year-2018,month-9,starttime-43345.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/3/archivelist/year-2018,month-9,starttime-43346.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/4/archivelist/year-2018,month-9,starttime-43347.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/5/archivelist/year-2018,month-9,starttime-43348.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/6/archivelist/year-2018,month-9,starttime-43349.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/7/archivelist/year-2018,month-9,starttime-43350.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/8/archivelist/year-2018,month-9,starttime-43351.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/9/archivelist/year-2018,month-9,starttime-43352.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/10/archivelist/year-2018,month-9,starttime-43353.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/11/archivelist/year-2018,month-9,starttime-43354.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/12/archivelist/year-2018,month-9,starttime-43355.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/13/archivelist/year-2018,month-9,starttime-43356.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/14/archivelist/year-2018,month-9,starttime-43357.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/15/archivelist/year-2018,month-9,starttime-43358.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/16/archivelist/year-2018,month-9,starttime-43359.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/17/archivelist/year-2018,month-9,starttime-43360.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/18/archivelist/year-2018,month-9,starttime-43361.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/19/archivelist/year-2018,month-9,starttime-43362.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/20/archivelist/year-2018,month-9,starttime-43363.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/21/archivelist/year-2018,month-9,starttime-43364.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/22/archivelist/year-2018,month-9,starttime-43365.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/23/archivelist/year-2018,month-9,starttime-43366.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/24/archivelist/year-2018,month-9,starttime-43367.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/25/archivelist/year-2018,month-9,starttime-43368.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/26/archivelist/year-2018,month-9,starttime-43369.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/27/archivelist/year-2018,month-9,starttime-43370.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/28/archivelist/year-2018,month-9,starttime-43371.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/29/archivelist/year-2018,month-9,starttime-43372.cms',\n",
       " 'https://timesofindia.indiatimes.com/2018/9/30/archivelist/year-2018,month-9,starttime-43373.cms']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************Collecting all news URL's on page  1 ***********************\n",
      "************* All news URL's on page  1  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  2 ***********************\n",
      "************* All news URL's on page  2  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  3 ***********************\n",
      "************* All news URL's on page  3  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  4 ***********************\n",
      "************* All news URL's on page  4  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  5 ***********************\n",
      "************* All news URL's on page  5  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  6 ***********************\n",
      "************* All news URL's on page  6  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  7 ***********************\n",
      "************* All news URL's on page  7  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  8 ***********************\n",
      "************* All news URL's on page  8  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  9 ***********************\n",
      "************* All news URL's on page  9  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  10 ***********************\n",
      "************* All news URL's on page  10  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  11 ***********************\n",
      "************* All news URL's on page  11  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  12 ***********************\n",
      "************* All news URL's on page  12  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  13 ***********************\n",
      "************* All news URL's on page  13  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  14 ***********************\n",
      "************* All news URL's on page  14  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  15 ***********************\n",
      "************* All news URL's on page  15  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  16 ***********************\n",
      "************* All news URL's on page  16  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  17 ***********************\n",
      "************* All news URL's on page  17  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  18 ***********************\n",
      "************* All news URL's on page  18  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  19 ***********************\n",
      "************* All news URL's on page  19  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  20 ***********************\n",
      "************* All news URL's on page  20  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  21 ***********************\n",
      "************* All news URL's on page  21  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  22 ***********************\n",
      "************* All news URL's on page  22  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  23 ***********************\n",
      "************* All news URL's on page  23  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  24 ***********************\n",
      "************* All news URL's on page  24  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  25 ***********************\n",
      "************* All news URL's on page  25  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  26 ***********************\n",
      "************* All news URL's on page  26  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  27 ***********************\n",
      "************* All news URL's on page  27  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  28 ***********************\n",
      "************* All news URL's on page  28  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  29 ***********************\n",
      "************* All news URL's on page  29  have been stored successfully**********\n",
      "**************Collecting all news URL's on page  30 ***********************\n",
      "************* All news URL's on page  30  have been stored successfully**********\n"
     ]
    }
   ],
   "source": [
    "################### traversing each date and collecting URL ###########################\n",
    "news_urls=[]\n",
    "cnt=1\n",
    "for j in dates:\n",
    "    browser.get(j)\n",
    "    print(\"**************Collecting all news URL's on page \",cnt,\"***********************\")\n",
    "    tble=browser.find_element_by_xpath(\"//table[@class='cnt'][2]\")\n",
    "    temp=tble.find_elements_by_xpath(\".//tbody/tr[2]/td[1]/div[3]/table/tbody/tr[2]/td\")\n",
    "    for i in temp[0].find_elements_by_xpath(\".//span/a\"):\n",
    "        news_urls.append(i.get_attribute('href'))\n",
    "    for i in temp[2].find_elements_by_xpath(\".//span/a\"):\n",
    "        news_urls.append(i.get_attribute('href'))\n",
    "    print(\"************* All news URL's on page \",cnt, \" have been stored successfully**********\")\n",
    "    cnt=cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(urls_):\n",
    "    author=[]\n",
    "    date=[]\n",
    "    vertical=[]\n",
    "    content=[]\n",
    "    heading=[]\n",
    "    ######################## traversing each URL one by one #########################\n",
    "    for k in urls_:\n",
    "        browser.get(k)\n",
    "        #******************** auhtor ************************\n",
    "        try:\n",
    "            txt=browser.find_element_by_xpath(\"//div[@class='as_byline']/div[2]\").text\n",
    "            if(txt.find('IST')==-1): author.append(txt)\n",
    "            else: \n",
    "                author.append('')\n",
    "                #date.append(txt.replace('Updated:','').replace('Created:',''))\n",
    "        except: \n",
    "            try:author.append(browser.find_element_by_xpath(\"//div[@class='_3Mkg- byline']/span\").text)\n",
    "            except:author.append('')\n",
    "            \n",
    "        #********************** dates **************************\n",
    "        try:\n",
    "            dt=browser.find_element_by_xpath(\"//div[@class='as_byline']/div[3]\").text.replace('Updated:','').replace('Created:','')\n",
    "            if(dt.find('IST')):date.append(dt)\n",
    "            else: date.append('')\n",
    "        except: \n",
    "            try:\n",
    "                dt=browser.find_element_by_xpath(\"//div[@class='as_byline']/div[2]\").text\n",
    "                if(txt.find('IST')!=-1):date.append(dt.replace('Updated:','').replace('Created:',''))\n",
    "                else:date.append('')  \n",
    "            except:\n",
    "                try:date.append(browser.find_element_by_xpath(\"//div[@class='_3Mkg- byline']\").text[-23:])\n",
    "                except:date.append('')\n",
    "                \n",
    "        #**************************** vertical ********************      \n",
    "        try: vertical.append(browser.find_element_by_xpath(\"//div[@class='navbdcrumb']/div[@class='wrapper']/ol/li[2]\").text)\n",
    "        except: vertical.append('')\n",
    "            \n",
    "        #*************************** content ***********************\n",
    "        try: content.append(browser.find_element_by_xpath(\"//div[@class='article_content clearfix']/arttextxml/div[@class='section1']/div[@class='Normal']\").text.replace('\\n',''))\n",
    "        except: \n",
    "            try:content.append(browser.find_element_by_xpath(\"//div[@class='ga-headlines']\").text.replace('\\n',''))\n",
    "            except:content.append('')\n",
    "                \n",
    "        #***************************** heading *********************\n",
    "        try: heading.append(browser.find_element_by_xpath(\"//div[@class='as_heading']/h1[@class='heading1']\").text)\n",
    "        except: \n",
    "            try:heading.append(browser.find_element_by_xpath(\"//div[@class='_2NFXP ']/h1[@class='_23498']\").text)\n",
    "            except:heading.append('')\n",
    "    df=pd.DataFrame({'Date':date,'Author':author,'Vertical':vertical,'Headline':heading,'Description':content})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=func(news_urls)\n",
    "df.to_csv('TOI_archive_data_sep2018.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
